{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13435c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.21-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Using cached jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Using cached jaxlib-0.8.0-cp312-cp312-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Using cached matplotlib-3.10.7-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.10.20-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "  Using cached mediapipe-0.10.18-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "  Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.3-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\onedrive\\desktop\\air writing\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.1-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\onedrive\\desktop\\air writing\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-2.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.17.0-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting scipy>=1.13 (from jax->mediapipe)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe)\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib->mediapipe)\n",
      "  Using cached pillow-12.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->mediapipe)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\onedrive\\desktop\\air writing\\.venv\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\onedrive\\desktop\\air writing\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached opencv_python-4.12.0.88-cp37-abi3-win_amd64.whl (39.0 MB)\n",
      "Using cached mediapipe-0.10.14-cp312-cp312-win_amd64.whl (50.8 MB)\n",
      "Using cached protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached tensorflow-2.19.1-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Using cached grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp312-cp312-win_amd64.whl (208 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "Using cached keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sounddevice-0.5.3-py3-none-win_amd64.whl (364 kB)\n",
      "Using cached cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached wrapt-2.0.0-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Using cached jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
      "Using cached jaxlib-0.8.0-cp312-cp312-win_amd64.whl (59.3 MB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "Using cached matplotlib-3.10.7-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-win_amd64.whl (2.3 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-win_amd64.whl (73 kB)\n",
      "Using cached pillow-12.0.0-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached opencv_contrib_python-4.12.0.88-cp37-abi3-win_amd64.whl (45.3 MB)\n",
      "Using cached optree-0.17.0-cp312-cp312-win_amd64.whl (314 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorboard-data-server, setuptools, pyparsing, pycparser, protobuf, pillow, opt_einsum, numpy, mdurl, MarkupSafe, markdown, kiwisolver, idna, google_pasta, gast, fonttools, cycler, charset_normalizer, certifi, attrs, absl-py, werkzeug, scipy, requests, optree, opencv-python, opencv-contrib-python, ml-dtypes, markdown-it-py, h5py, grpcio, contourpy, CFFI, astunparse, tensorboard, sounddevice, rich, matplotlib, jaxlib, keras, jax, tensorflow, mediapipe\n",
      "\n",
      "    ---------------------------------------  1/51 [libclang]\n",
      "    ---------------------------------------  1/51 [libclang]\n",
      "    ---------------------------------------  1/51 [libclang]\n",
      "    ---------------------------------------  1/51 [libclang]\n",
      "   - --------------------------------------  2/51 [flatbuffers]\n",
      "   -- -------------------------------------  3/51 [wrapt]\n",
      "   --- ------------------------------------  4/51 [wheel]\n",
      "   --- ------------------------------------  4/51 [wheel]\n",
      "   --- ------------------------------------  4/51 [wheel]\n",
      "   --- ------------------------------------  4/51 [wheel]\n",
      "   --- ------------------------------------  5/51 [urllib3]\n",
      "   --- ------------------------------------  5/51 [urllib3]\n",
      "   --- ------------------------------------  5/51 [urllib3]\n",
      "   --- ------------------------------------  5/51 [urllib3]\n",
      "   --- ------------------------------------  5/51 [urllib3]\n",
      "   ---- -----------------------------------  6/51 [typing-extensions]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- --------------------------------  9/51 [setuptools]\n",
      "   ------- -------------------------------- 10/51 [pyparsing]\n",
      "   ------- -------------------------------- 10/51 [pyparsing]\n",
      "   -------- ------------------------------- 11/51 [pycparser]\n",
      "   -------- ------------------------------- 11/51 [pycparser]\n",
      "   -------- ------------------------------- 11/51 [pycparser]\n",
      "   --------- ------------------------------ 12/51 [protobuf]\n",
      "   --------- ------------------------------ 12/51 [protobuf]\n",
      "   --------- ------------------------------ 12/51 [protobuf]\n",
      "   --------- ------------------------------ 12/51 [protobuf]\n",
      "   --------- ------------------------------ 12/51 [protobuf]\n",
      "   ---------- ----------------------------- 13/51 [pillow]\n",
      "   ---------- ----------------------------- 13/51 [pillow]\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\Air Writing\\\\.venv\\\\Lib\\\\site-packages\\\\PIL\\\\WalImageFile.py'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python mediapipe tensorflow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ad6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TensorFlow version: 2.19.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"âœ… TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71636e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully.\n",
      "ðŸŽ¯ Air-writing Digit Recognition Started. Press 'q' to quit, 'c' to clear canvas, 'p' to predict digit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive\\Desktop\\Air Writing\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras.datasets import mnist # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D # type: ignore\n",
    "\n",
    "# ========================\n",
    "# Step 1: Prepare/load MNIST model\n",
    "# ========================\n",
    "\n",
    "try:\n",
    "    model = load_model(\"mnist_model.h5\")\n",
    "    print(\"âœ… Model loaded successfully.\")\n",
    "except:\n",
    "    print(\"âš ï¸ Model not found. Training new model...\")\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=15,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "    model.save(\"mnist_model.h5\")\n",
    "    print(\"âœ… Model trained and saved.\")\n",
    "\n",
    "    \n",
    "\n",
    "# ========================\n",
    "# Step 2: Hand Tracking Setup\n",
    "# ========================\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# ========================\n",
    "# Step 3: Drawing Canvas\n",
    "# ========================\n",
    "\n",
    "canvas = np.zeros((512,512), dtype=np.uint8)\n",
    "drawing = False\n",
    "prev_x, prev_y = None, None\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"ðŸŽ¯ Air-writing Digit Recognition Started. Press 'q' to quit, 'c' to clear canvas, 'p' to predict digit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for handLms in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "            x = int(handLms.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * 512)\n",
    "            y = int(handLms.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * 512)\n",
    "            if prev_x is None:\n",
    "                prev_x, prev_y = x, y\n",
    "            cv2.line(canvas, (prev_x, prev_y), (x, y), 255, 10)\n",
    "            prev_x, prev_y = x, y\n",
    "    else:\n",
    "        prev_x, prev_y = None, None\n",
    "\n",
    "    cv2.imshow(\"Air Canvas\", canvas)\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        canvas = np.zeros((512,512), dtype=np.uint8)\n",
    "    elif key == ord('p'):\n",
    "        # Preprocess canvas and predict\n",
    "        img = cv2.resize(canvas, (28,28))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = img.reshape(1,28,28,1)\n",
    "        pred = np.argmax(model.predict(img), axis=1)[0]\n",
    "        print(f\"ðŸŽ‰ Predicted Digit: {pred}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0309\n",
      "Test Accuracy: 0.9909999966621399\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist # type: ignore\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "\n",
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test.reshape(-1,28,28,1)/255.0\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"mnist_model.h5\")\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a999240a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw digit: 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Expected: 0, Predicted: 7\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Expected: 1, Predicted: 2\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Expected: 2, Predicted: 7\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 3\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Expected: 3, Predicted: 4\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 4\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Expected: 4, Predicted: 2\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Expected: 5, Predicted: 1\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 6\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Expected: 6, Predicted: 1\n",
      "Real-time accuracy: 0.00%\n",
      "Now draw: 7\n",
      "Now draw: 8\n",
      "Now draw: 9\n",
      "Now draw: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m frame = cv2.flip(frame, \u001b[32m1\u001b[39m)\n\u001b[32m     31\u001b[39m rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m result = \u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.multi_hand_landmarks:\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m handLms \u001b[38;5;129;01min\u001b[39;00m result.multi_hand_landmarks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\OneDrive\\Desktop\\Air Writing\\.venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\OneDrive\\Desktop\\Air Writing\\.venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model # type: ignore\n",
    "import time\n",
    "\n",
    "model = load_model(\"mnist_model.h5\")\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "canvas = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "digits_to_draw = [0,1,2,3,4,5,6,7,8,9]  # sequence of targets\n",
    "current_digit = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "prev_x, prev_y = 0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Draw digit: {digits_to_draw[current_digit]}\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for handLms in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "            x = int(handLms.landmark[8].x * 640)\n",
    "            y = int(handLms.landmark[8].y * 480)\n",
    "\n",
    "            if prev_x == 0 and prev_y == 0:\n",
    "                prev_x, prev_y = x, y\n",
    "            cv2.line(canvas, (prev_x, prev_y), (x, y), (255, 255, 255), 8)\n",
    "            prev_x, prev_y = x, y\n",
    "\n",
    "    # Every 5 seconds, evaluate prediction\n",
    "    if time.time() - start_time > 5:\n",
    "        gray = cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            cnt = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            digit = thresh[y:y+h, x:x+w]\n",
    "            digit = cv2.resize(digit, (28,28))\n",
    "            digit = digit.reshape(1,28,28,1)/255.0\n",
    "            pred = model.predict(digit)\n",
    "            y_pred = np.argmax(pred)\n",
    "\n",
    "            # Compare prediction\n",
    "            y_true = digits_to_draw[current_digit]\n",
    "            total += 1\n",
    "            if y_pred == y_true:\n",
    "                correct += 1\n",
    "\n",
    "            print(f\"Expected: {y_true}, Predicted: {y_pred}\")\n",
    "            print(f\"Real-time accuracy: {(correct/total)*100:.2f}%\")\n",
    "\n",
    "        # Prepare for next digit\n",
    "        current_digit = (current_digit + 1) % len(digits_to_draw)\n",
    "        print(f\"Now draw: {digits_to_draw[current_digit]}\")\n",
    "        start_time = time.time()\n",
    "        canvas = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        prev_x, prev_y = 0, 0\n",
    "\n",
    "    combined = cv2.addWeighted(frame, 0.5, canvas, 0.5, 0)\n",
    "    cv2.imshow(\"Air Writing Digit\", combined)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
